<img src="https://github.com/joaopaulo-souza/Introduction-to-AI/blob/master/Images/egg-robot.jpeg" width="900">

<h1>Basic Concepts of AI</h1>

<h2>Data Science</h2>
Data science is an interdisciplinary field focused on analyzing, interpreting, and modeling structured and unstructured data. It integrates techniques from statistics, mathematics, computer science, to extract meaningful insights and useful knowledge from data.

<h2>Artificial Intelligence</h2>
Artificial intelligence is an interdisciplinary field dedicated to building models and algorithms that perform tasks requiring human-like intelligence. These tasks include learning, reasoning, problem-solving, perception, and natural language understanding. AI and data science often overlap and intersect, as both fields aim to derive value from data and build intelligent systems.

<h2>Machine Learning</h2>
Machine learning is a subfield of artificial intelligence and data science. It focuses on developing models that learn from and make predictions or decisions based on training data. Machine learning algorithms enable systems to improve their performance over time without being explicitly programmed for specific tasks.

<h2>Deep Learning</h2>
Deep learning is a subfield of machine learning that aims to build models using various types of neural networks. These neural networks are designed to automatically learn and represent complex patterns in data, enabling advanced tasks such as image recognition, natural language processing, and more. Deep learning has been instrumental in achieving breakthroughs in AI by leveraging large datasets and computational power.

<h2>Bias</h2>
It's a characteristic of a simple model in face of the complexity of a real world problem. For instance: approximating a non-linear problem by a linear one, introduces an error. This error is the bias.   
High Bias: Leads to underfitting, where the model is too simple to capture the underlying patterns in the data.

<h2>Variance</h2>
A characteristic of a more complex model: It's the error introduced by the model's sensitivity to small fluctuations in the training set.
High Variance: Leads to overfitting, where the model captures noise in the training data instead of the actual patterns. The problem is that the model perform very poorly in test data. 


<h2>Trade-off</h2>
Bias-Variance Trade-off: The balance between bias and variance to achieve optimal model performance. Reducing one typically increases the other.


<h2>Overfitting</h2>
When a model learns the details and noise in the training data to the extent that it negatively impacts the model's performance on new data.
Indicators: High accuracy on training data but poor accuracy on validation/test data.


<h2>Underfitting</h2>
When a model is too simple to capture the underlying structure of the data.
Indicators: Poor accuracy on both training and validation/test data.